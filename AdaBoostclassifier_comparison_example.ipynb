{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangou888/NN/blob/main/AdaBoostclassifier_comparison_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad35ace-109d-4ee5-b141-aac1a1ea558b",
      "metadata": {
        "id": "bad35ace-109d-4ee5-b141-aac1a1ea558b"
      },
      "source": [
        "# AdaBoost classifier example (2 adaboost classifier comparison)\n",
        "\n",
        "**AdaBoost (Adaptive Boosting)** is an ensemble learning method that combines multiple weak learners (typically decision trees with a maximum depth of 1) to create a strong learner.\n",
        "\n",
        "It works by iteratively training weak learners, with each learner focusing on the instances that were misclassified by the previous learners.\n",
        "\n",
        "AdaBoost assigns weights to both the training instances and the weak learners. Instances that are difficult to classify receive higher weights, and more accurate learners receive higher weights in the final ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5599a13d-70cd-4544-a438-99dbf25eb7ab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:47:05.929125Z",
          "iopub.status.busy": "2025-07-07T19:47:05.928744Z",
          "iopub.status.idle": "2025-07-07T19:47:05.932223Z",
          "shell.execute_reply": "2025-07-07T19:47:05.931849Z",
          "shell.execute_reply.started": "2025-07-07T19:47:05.929108Z"
        },
        "id": "5599a13d-70cd-4544-a438-99dbf25eb7ab"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdf0181-07d1-4428-aeca-ff945587b08b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:23:34.421597Z",
          "iopub.status.busy": "2025-07-07T19:23:34.421269Z",
          "iopub.status.idle": "2025-07-07T19:23:34.426852Z",
          "shell.execute_reply": "2025-07-07T19:23:34.426417Z",
          "shell.execute_reply.started": "2025-07-07T19:23:34.421581Z"
        },
        "id": "ccdf0181-07d1-4428-aeca-ff945587b08b"
      },
      "outputs": [],
      "source": [
        "# 1. Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=15, n_redundant=5,\n",
        "                           random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6ccbe4-2226-4b3e-be1b-7395d18e610e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:23:34.831616Z",
          "iopub.status.busy": "2025-07-07T19:23:34.831008Z",
          "iopub.status.idle": "2025-07-07T19:23:34.834597Z",
          "shell.execute_reply": "2025-07-07T19:23:34.834162Z",
          "shell.execute_reply.started": "2025-07-07T19:23:34.831599Z"
        },
        "id": "5e6ccbe4-2226-4b3e-be1b-7395d18e610e"
      },
      "outputs": [],
      "source": [
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00aac04e-0ecf-4456-854b-16c0bff730c2",
      "metadata": {
        "id": "00aac04e-0ecf-4456-854b-16c0bff730c2"
      },
      "source": [
        "## Approach 1: Basic AdaBoostClassifier with Decision Trees (Stumps)\n",
        "-\n",
        "Descriptio\n",
        "  : This is the \"classic\" AdaBoost setup. It uses decision trees with a maximum depth of 1 (often call d \"decision stumps\") as the weak learners. AdaBoost iteratively trains these stumps, weighti g misclassified instances higher in each iteration. The final prediction is a weighted combination of t e predictions from all the stump\n",
        ".-\n",
        "\n",
        "Pr    - s:\n",
        "\n",
        "Simple and fast to train: Decision stumps are very simple, so each iteration of AdaBoost is relatively q    - uick.\n",
        "Few parameters to tune: The main parameters are the number of estimators (n_estimators) and the learning rate (learning_    - rate).\n",
        "Good for high-dimensional data: Can handle data with many features reasonabl    - y well.\n",
        "Resistant to overfitting (to a point): AdaBoost can be less prone to overfitting than a single complex decision tree, especially in lower dimensional    -  spaces.\n",
        "Easy to interpret: Decision stumps are inherently inter\n",
        "- pretabl    - e.\n",
        "Cons:\n",
        "\n",
        "Weak learners may limit accuracy: Decision stumps are very weak learners. While AdaBoost combines them effectively, the overall accuracy might be limited, especially on compl    - ex datasets.\n",
        "Sensitive to noisy data and outliers: AdaBoost can be sensitive to noisy data because it focuses on misclassified instances. Outliers can receive high weights and disproportionately influen    - ce the model.\n",
        "Can underfit: May not be able to capture complex relationships in the data, leading to    -  underfitting.\n",
        "May require careful tuning of n_estimators and learning_rate: Selecting the right combination is important. Too few estimators can lead to underfitting; too many, or a learning rate that is too high, can lead to overfitting, especially if the\n",
        "-  data is noisy.\n",
        "Suit    - able Use Cases:\n",
        "\n",
        "Binary classification problems where speed and simplic    - ity are important.\n",
        "High-dimensional datasets where interpret    - ability is desired.\n",
        "Situations where a quick baseline model is needed before trying more    -  complex approaches.\n",
        "Problems where the relationships between features and target are relatively simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d0abd1-4fda-425d-9e06-de9c9c9061a8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:28:29.522580Z",
          "iopub.status.busy": "2025-07-07T19:28:29.521769Z",
          "iopub.status.idle": "2025-07-07T19:28:29.724247Z",
          "shell.execute_reply": "2025-07-07T19:28:29.723733Z",
          "shell.execute_reply.started": "2025-07-07T19:28:29.522554Z"
        },
        "id": "e2d0abd1-4fda-425d-9e06-de9c9c9061a8",
        "outputId": "d4e872a2-413b-40a8-b3d9-3e3e5751a096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approach 1: Basic AdaBoost with Decision Trees\n",
            "Accuracy: 0.8366666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       160\n",
            "           1       0.82      0.84      0.83       140\n",
            "\n",
            "    accuracy                           0.84       300\n",
            "   macro avg       0.84      0.84      0.84       300\n",
            "weighted avg       0.84      0.84      0.84       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Approach 1: Basic AdaBoostClassifier with Decision Trees (as before)\n",
        "ada_boost_dt = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "ada_boost_dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = ada_boost_dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "print(\"Approach 1: Basic AdaBoost with Decision Trees\")\n",
        "print(f\"Accuracy: {accuracy_dt}\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018aba26-9d9a-4895-8ee4-c4b525fa06a7",
      "metadata": {
        "id": "018aba26-9d9a-4895-8ee4-c4b525fa06a7"
      },
      "source": [
        "## Approach 2: AdaBoost with Support Vector Machine (SVM) and Hyperparameter Tuning\n",
        "- Description: This approach uses Support Vector Machines (SVMs) as the weak learners. SVMs are more powerful and flexible than decision stumps, but they are also more complex and require careful tuning. Hyperparameter tuning (using GridSearchCV in the example) is crucial to optimize the SVM's parameters. Because SVMs are sensitive to feature scaling, a Pipeline with StandardScaler is used to ensure proper preprocessing.\n",
        "-\n",
        "Pros:    -\n",
        "\n",
        "Potentially higher accuracy: SVMs are capable of capturing more complex relationships in the data than decision stum    - ps.\n",
        "Flexibility: SVMs have different kernel functions (linear, RBF, polynomial, etc.) that can be chosen to suit the d    - ata.\n",
        "Good for complex datasets: Can handle datasets with non-linear relationships between features and the ta    - rget.\n",
        "Regularization: SVMs have a regularization parameter (C) that helps prevent overfi\n",
        "- tting.\n",
        "    -\n",
        "Cons:\n",
        "\n",
        "Slower to train: SVMs are generally slower to train than decision stumps, especially on large datasets. The hyperparameter tuning process adds even more computatio    - nal cost.\n",
        "More parameters to tune: SVMs have several hyperparameters that need to be tuned, such as the kernel type, regularization parameter (C), and kernel coefficien    - t (gamma).\n",
        "More prone to overfitting: SVMs are more prone to overfitting than decision stumps, especially if the hyperparameters are not tune    - d properly.\n",
        "Less interpretable: SVMs are generally less interpretable than decision trees. The decision boundary learned by an SVM can be difficult to visualize and    -  understand.\n",
        "Requires careful preprocessing: SVMs are sensitive to feature scaling, so it's important to scale the data bef    - ore training.\n",
        "Choice of kernel is critical: The \"right\" kernel depends heavily on the data; if the data is naturally linear, a linear kernel is often best, but for more complex data, RBF or polynomial kernels\n",
        "- may be needed.\n",
        "Suita- ble Use Cases:\n",
        "\n",
        "Problems where high accuracy is the primary goal, even if it comes at the cost of increased training tim- e and complexity.\n",
        "Datasets with complex, non-linear relationships between featur- es and the target.\n",
        "Situations where interpretability is n- ot a major concern.\n",
        "Problems where feature scaling is possible and can - improve performance.\n",
        "When computational resources are available to perform hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2ad433-fa47-4b55-b949-1bc6e99f87b7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:49:15.247571Z",
          "iopub.status.busy": "2025-07-07T19:49:15.246893Z",
          "iopub.status.idle": "2025-07-07T19:49:16.014915Z",
          "shell.execute_reply": "2025-07-07T19:49:16.014325Z",
          "shell.execute_reply.started": "2025-07-07T19:49:15.247551Z"
        },
        "id": "cf2ad433-fa47-4b55-b949-1bc6e99f87b7",
        "outputId": "2eeaa241-df1c-4b83-aa4b-647fe5c60511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tuning the SVC:\n",
            "Best parameters (for the Pipeline/SVM): {'svm__C': 1, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
            "Best score (for the Pipeline/SVM): 0.9342834085323356\n",
            "Accuracy (Stacked AdaBoost with tuned SVC): 0.9533333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.96       160\n",
            "           1       0.93      0.98      0.95       140\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.95      0.95      0.95       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Tune the SVC separately\n",
        "# Create a pipeline with StandardScaler and SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Feature scaling\n",
        "    ('svm', SVC(probability=True, random_state=42))  # SVC needs probability=True for AdaBoost\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV - for the SVM *within the pipeline*\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],  # Regularization parameter\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Kernel type\n",
        "    'svm__gamma': ['scale', 'auto']  # Kernel coefficient\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning - DIRECTLY ON THE PIPELINE\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)  # n_jobs=-1 uses all available cores\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score - FOR THE PIPELINE\n",
        "print(\"\\nTuning the SVC:\")\n",
        "print(\"Best parameters (for the Pipeline/SVM):\", grid_search.best_params_)\n",
        "print(\"Best score (for the Pipeline/SVM):\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator from the grid search - THIS IS THE TUNED PIPELINE\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# 4. Train the tuned SVC and get predictions\n",
        "# Train the best pipeline on the *entire* training set\n",
        "best_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Get the probability predictions from the tuned SVC\n",
        "train_predictions = best_pipeline.predict_proba(X_train)\n",
        "test_predictions = best_pipeline.predict_proba(X_test)\n",
        "\n",
        "# 5. Create a *new* AdaBoost model using the SVC predictions as features\n",
        "# The new AdaBoost model will use the predictions of the tuned SVC as features\n",
        "ada_boost = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the AdaBoost model using the SVC predictions as features\n",
        "ada_boost.fit(train_predictions, y_train)\n",
        "\n",
        "# Make predictions using the AdaBoost model\n",
        "y_pred = ada_boost.predict(test_predictions)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy (Stacked AdaBoost with tuned SVC): {accuracy_svm}\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f796af91-bc09-4ec1-af3a-eb6fdfb430ca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-07T19:49:19.389446Z",
          "iopub.status.busy": "2025-07-07T19:49:19.389064Z",
          "iopub.status.idle": "2025-07-07T19:49:19.393179Z",
          "shell.execute_reply": "2025-07-07T19:49:19.392720Z",
          "shell.execute_reply.started": "2025-07-07T19:49:19.389427Z"
        },
        "id": "f796af91-bc09-4ec1-af3a-eb6fdfb430ca",
        "outputId": "52154de1-c489-4b4b-dc6d-80540468aeff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparison:\n",
            "Approach 1 (Decision Tree): Accuracy = 0.8366666666666667\n",
            "Approach 2 (SVM with Tuning): Accuracy = 0.9533333333333334\n",
            "\n",
            "Discussion:\n",
            "The choice of base estimator and hyperparameter tuning significantly impacts performance.\n",
            "SVMs, being more complex, can potentially achieve higher accuracy but require careful tuning.\n",
            "The Pipeline ensures proper scaling of data before being fed to the SVM.\n",
            "GridSearchCV automates the process of finding the best hyperparameters for the SVM within the AdaBoost framework.\n"
          ]
        }
      ],
      "source": [
        "# 5. Comparison and Discussion\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Approach 1 (Decision Tree): Accuracy = {accuracy_dt}\")\n",
        "print(f\"Approach 2 (SVM with Tuning): Accuracy = {accuracy_svm}\")\n",
        "\n",
        "print(\"\\nDiscussion:\")\n",
        "print(\"The choice of base estimator and hyperparameter tuning significantly impacts performance.\")\n",
        "print(\"SVMs, being more complex, can potentially achieve higher accuracy but require careful tuning.\")\n",
        "print(\"The Pipeline ensures proper scaling of data before being fed to the SVM.\")\n",
        "print(\"GridSearchCV automates the process of finding the best hyperparameters for the SVM within the AdaBoost framework.\")\n",
        "\n",
        "#Add more comparison based on your observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca3f86b9-b904-460b-94c4-9fe8fc25724c",
      "metadata": {
        "id": "ca3f86b9-b904-460b-94c4-9fe8fc25724c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds-python",
      "language": "python",
      "name": "ds-python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}